{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string #string operations\n",
    "import re #useful for regular expressions\n",
    "import nltk # to have the lemmatizer and stemmer\n",
    "import time #measure time between some operations, to have a metric\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from utils import load_json, load_pkl, save_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#from __future__ import print_function\n",
    "\n",
    "from lab04_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allCourses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def pickleDump(filename, value):\n",
    "#     \"\"\"Save an object (e.g. table) to a pickle file to be readable by other notebooks\"\"\"\n",
    "#     with open(filename, \"wb\") as f:\n",
    "#         pk.dump(value, f)\n",
    "        \n",
    "def listPrettyPrint(l, n):\n",
    "    \"\"\"Prints a list l on n columns to improve readability\"\"\"\n",
    "    if(n == 4):\n",
    "        for a,b,c,d in zip(l[::4],l[1::4],l[2::4],l[3::4]):\n",
    "            print('{:<30}{:<30}{:<30}{:<}'.format(a,b,c,d))\n",
    "    if(n == 3):\n",
    "        for a,b,c in zip(l[::3],l[1::3],l[2::3]):\n",
    "            print('{:<30}{:<30}{:<}'.format(a,b,c))\n",
    "    if(len(l)%n != 0): #print remaining\n",
    "        for i in range(len(l)%n):\n",
    "            print(l[-(len(l)%n):][i], end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To have a complete list of punctions, please see the *lab_04_helper.py* file.\n",
    "To avoid errors, we preferred to have the same preprocessing function available for the notebook 2 (LSI), in order to perform term searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the following functions :\n",
    "   * **Stopwords** : Obviously, terms as \"*the*\", \"*it*\" and such do not need to appear in our bag of words. They will only cause useless noise, as they appear a lot but in (almost) every description.\n",
    "   * **Taking numbers out** : this was a decision we made. A lot of numbers are useless (such as describing time) but some are far from useless (e.g. \"*3SAT*\"). Therefore, we decided to remove lone numbers or when separated by the character *h*. This already removes most of the noise.\n",
    "   * **Split appended** : Not quite a word-filtering function but still preprocesses. Due to the scraping method used, words ending and beginning a line (originally separated by a *\\n*) were stuck together. Our function separates these, based on the presence of an uppercase charactes in the middle of a word. Sentences starting with a lower case character (thus creating fused words with only lower cases) could not be split.\n",
    "   * **Punctuation** : \"*word*\", \"*word!*\" and \"*word.*\" should be treated equally ; furthermore, punctuation signs standing on their own (not appended to a word) will create a unique word, which should not be the case. Thus, we removed all punctuation sign (see complete list in the *helper* file)\n",
    "   * **Lower cased** : our system should not be case-sensitive, so once capital letters are not important anymore (as in splitting), we put all letters to lower case. \n",
    "   \n",
    "These functions are called in a careful order to produce our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of a dictionary corpusDict that contains :\n",
    "# - courses ID as keys\n",
    "# - a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "#indexCourse has indices as keys that link to their course (bijection mapping)\n",
    "ref_corpus = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for c in allCourses:\n",
    "    if c['courseId'] not in ref_corpus.keys(): #avoids situation where courses are tripled\n",
    "        ref_corpus[c['courseId']] = (index, c['name'], cleaner(c['description']))\n",
    "        indexCourse[index] = c['courseId']\n",
    "        index += 1\n",
    "        \n",
    "print(\"We are working with a total of %d courses\" % len(ref_corpus))\n",
    "save_pkl(ref_corpus, r\"cidWithBag.txt\")\n",
    "save_pkl(indexCourse, r\"indexToCourse.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixWords = sorted(ref_corpus['COM-308'][2])\n",
    "print(\"Words for Internet Analytics course are (in alphabetical order) :\")\n",
    "listPrettyPrint(ixWords, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : some word seem strange as they have been stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordToIndex contains all distinct words as keys and their unique index as value\n",
    "#indexToWord is the exact opposite. \n",
    "wordToIndex = dict() \n",
    "index = 0\n",
    "for name in ref_corpus:\n",
    "    for word in ref_corpus[name][2]:\n",
    "        if word not in wordToIndex.keys():\n",
    "            wordToIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexToWord = dict((v, k) for k, v in wordToIndex.items())\n",
    "print(\"After preprocessing, we are dealing with a total of %d \\\"unique\\\" words\" % len(indexToWord))\n",
    "save_pkl(indexToWord,r\"indexToWord\")\n",
    "save_pkl( wordToIndex,r\"wordToIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of sparse occurence matrix. we define values in occValues and its indices in occRow and occCol. \n",
    "#If two pairs of indices are identical, their values will be added. \n",
    "def findOccurenceMatrix(corpus):\n",
    "    \"\"\"\n",
    "    Takes a corpus of texts (cleaned) and outputs its occurence matrix \n",
    "    (number of times each word appears in each description of the corpus)\"\"\"\n",
    "    occValues = []\n",
    "    occRow = [] #indices of words\n",
    "    occCol = [] #indices of courses\n",
    "\n",
    "    i = 0\n",
    "    for cid in corpus: #iterate through all courses ID (and their bag of words)\n",
    "        cIndex = corpus[cid][0] #get column for this course\n",
    "        for word in corpus[cid][2]: #then append to correct list :\n",
    "            occCol.append(cIndex) #the col index\n",
    "            occRow.append(wordToIndex[word]) #row index\n",
    "            occValues.append(1) #value (1, as each word represents 1 occurence)\n",
    "    return csr_matrix((occValues, (occRow, occCol)), \n",
    "                      shape=((len(wordToIndex), len(corpus))), \n",
    "                      dtype=np.float64)\n",
    "\n",
    "def getTFIDF(corpus, occMatrix):\n",
    "    \"\"\"For a corpus of text and its occurence matrix, output the corresponding TF-IDF score matrix\n",
    "    \"\"\"\n",
    "    #for each course, keep only max value ≃ occurence of most frequent word\n",
    "    mostFreqWord = occMatrix.max(axis=0).data \n",
    "\n",
    "    #by definition : TF is term freq divided by freq of most freq word in the same document\n",
    "    TF = csr_matrix(occMatrix/mostFreqWord)\n",
    "    IDF = csr_matrix(-np.log2((occMatrix != 0).sum(1)/len(corpus)))\n",
    "    return TF.multiply(IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_occurenceMatrix = findOccurenceMatrix(ref_corpus)\n",
    "ref_TFIDF = getTFIDF(ref_corpus, ref_occurenceMatrix)\n",
    "\n",
    "#save for use in different botebooks\n",
    "np.save(\"TFIDF\", ref_TFIDF)\n",
    "save_sparse_csr(\"occ_matrix\", ref_occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## some work on COM-308\n",
    "ixIndex = ref_corpus['COM-308'][0]\n",
    "ix_col = ref_TFIDF.getcol(ixIndex)\n",
    "ixBigIndices = (np.argsort(ix_col.data, axis=0)[-15:]) #yields indices of the sorted values\n",
    "\n",
    "ixBigScores = [indexToWord[i] for i in ixBigIndices]\n",
    "ixBigScores.reverse()\n",
    "print(\"15 words with greatest scores are :\")\n",
    "listPrettyPrint(ixBigScores, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "The difference between high and big scores is the essence of TF-IDF : high scores indicate the term is very frequent in the document but appears in few documents, when a low score shows the word is rare in the document (appears only once or twice), but most documents have this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We create a \"new\" document with only the words we want, as a reference vector\n",
    "def sim(vec1, vec2):\n",
    "    \"\"\"Compute similarity (cosine of angle) between 2 vectors of same size\"\"\"\n",
    "    assert(len(vec1) == len(vec2))\n",
    "    prod_norm = norm(vec1)*norm(vec2)\n",
    "    prod_elem = np.asscalar(vec1.T*vec2)\n",
    "    return prod_elem/prod_norm\n",
    "\n",
    "def find5closest(query):\n",
    "    \"\"\"Finds the 5 documents with smallest angle (cosine closest to 1)\n",
    "    \n",
    "    For that, a copy of the reference corpus is made (to avoid the query affecting future queries),\n",
    "    then a new entry is added to this temp corpus (consider query as a new document) as before, with\n",
    "    a 'cleaned' query. Then the occurence matrix and TF-IDF are computed with this new 'document'\n",
    "    \n",
    "    This version is not the most efficient as it recomputes the whole TFIDF matrix for every query. But as the \n",
    "    corpus/# of words is not gigantic, this is acceptable\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    query -- A string\n",
    "    \"\"\"\n",
    "    new_corpus = ref_corpus.copy() #To not affect the ref corpus, make a temp copy\n",
    "    query_index = len(ref_corpus) #query goes at last index \n",
    "    new_corpus['query'] = (query_index, 'my_query', cleaner(query)) #add an entry to the corpus\n",
    "    query_occ_matrix = findOccurenceMatrix(new_corpus)\n",
    "    query_TFIDF = getTFIDF(new_corpus, query_occ_matrix)\n",
    "    \n",
    "    query_vec = query_TFIDF.getcol(query_index) # ≃ TFIDF score of the query  vector\n",
    "    \n",
    "    #compare the query vector to every other vectors\n",
    "    best_matches = []\n",
    "    for i in range(len(ref_corpus)):\n",
    "        simil = sim(query_vec.todense(), ref_TFIDF.getcol(i).todense())\n",
    "        if(simil > 0):\n",
    "            best_matches.append((i, simil))\n",
    "    sort = sorted(best_matches, key=lambda x : x[1])\n",
    "    if(len(sort) >= 5):\n",
    "        return sort[-5:]\n",
    "    else:\n",
    "        return sort\n",
    "    \n",
    "def print_closest(query, result):\n",
    "    \"\"\"Pretty printer to print queried results\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    query -- The query used to find the results (string)\n",
    "    result -- List of tuples (index, score) sorted in ascending score order\n",
    "    \"\"\"\n",
    "    print(\"The course(s) most relevant to '%s' is(are):\" % query)\n",
    "    for i in result:\n",
    "        print(\"\\t\",indexCourse[i[0]],\"(\",ref_corpus[indexCourse[i[0]]][1],\") with score\",i[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bef = time.time()\n",
    "markovClosest = find5closest(\"markov chains\")\n",
    "mid = time.time()\n",
    "facebookClosest = find5closest(\"facebook\")\n",
    "aft = time.time()\n",
    "\n",
    "print(\"%.4f s  for first query, %.4f s for second, %.4f s for both\" % (mid-bef, aft-mid, aft-bef))\n",
    "print_closest(\"markov chains\", markovClosest)\n",
    "\n",
    "print_closest(\"facebook\", facebookClosest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanations\n",
    "For markov chains, we can see 5 courses are found (actually 20). This is an expected result ; scores are quite high with such diversity, indicating the courses are tightely linked to our query.\n",
    "\n",
    "On the other hand, for facebook, only 1 course is found. This is normal, as this course (EE-727) is the only in the whole corpus to contain the word 'facebook'. Indeed, to obtain a similarity score with the query > 0, a course must contain at least one word (after 'cleaning') of the query. Here facebook being the only word, only courses with this word in them will appear."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
