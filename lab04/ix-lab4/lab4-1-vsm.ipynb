{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import lab04_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickleDump(filename, value):\n",
    "    with open(filename, \"w\") as f:\n",
    "        pk.dump(filename, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeStopWords(listWords):\n",
    "    \"\"\"\n",
    "    Filters out stopwords in a list of words\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x : len(x)>0 and x not in stopwords, listWords))\n",
    "\n",
    "def toWordList(description):\n",
    "    \"\"\"\n",
    "    takes a description (unique string) and separates it to lowercase words (not distincts) \n",
    "    \"\"\"\n",
    "    return description.lower().split(\" \") \n",
    "\n",
    "def takeOutNumbers(listWords):\n",
    "    \"\"\"\n",
    "    Takes a list of words and removes all numbers that are alone or a only seperated by h.\n",
    "    Permits to keep words that exist with a number (ex: 3SAT)\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"\\d{1,2}h\\d{0,2}$\") #removes hours\n",
    "    noHours = [pattern.sub(\"\", i) for i in listWords] \n",
    "    return list(filter(lambda x : not x.isdecimal(), noHours)) #filters out numbers-only\n",
    "\n",
    "def splitAppendedWords(descString):\n",
    "    \"\"\"\n",
    "    Takes a description, and splits appended words (because of a missing \\\\n)\n",
    "    \"\"\"\n",
    "    patternAppended = re.compile(\"([a-z][a-z])([A-Z])([a-z][a-z])\") #regex used to split bonded words \n",
    "    return patternAppended.sub(\"\\\\1 \\\\2\\\\3\", descString)\n",
    "\n",
    "def removePunctuation(descString):\n",
    "    \"\"\"\n",
    "    Removes punctuation signs in a long unique string. Treats dashes smartly.\n",
    "    \"\"\"\n",
    "    punct = \",.!?+\\n\\t:;0'%&\\\"#/()[]`\\xa0\\xad\" #list of characters that always need to be removed\n",
    "    puncttrans = str.maketrans(punct,\" \"*len(punct)) #translation rule : replace above char by a space\n",
    "    patternDash = re.compile(\" +- *| *- +\") # regex used to treat dashes\n",
    "    \n",
    "    unDashed = patternDash.sub(\"\", descString) \n",
    "    return unDashed.translate(puncttrans)\n",
    "\n",
    "\n",
    "\n",
    "def cleaner(oneCourse):\n",
    "    \"\"\"\n",
    "    Calls all above functions. First remove punctuation, then un-append words, split to space, then remove numbers\n",
    "    and stopwords. \n",
    "    \"\"\"\n",
    "    description = oneCourse['description']\n",
    "    noPunct = removePunctuation(description) #desc without punctuation\n",
    "    unAppended = splitAppendedWords(noPunct)  #desc with split words\n",
    "    \n",
    "    wordlist = toWordList(unAppended)\n",
    "    return removeStopWords(takeOutNumbers(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "explain why those functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of a dictionary that contains :\n",
    "#courses ID as keys\n",
    "#a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "descDict = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for i in courses:\n",
    "    if i['courseId'] not in descDict.keys():\n",
    "        descDict[i['courseId']] = (index, i['name'], cleaner(i))\n",
    "        indexCourse[index] = i['courseId']\n",
    "        index += 1\n",
    "with open(r\"cidWithBag.txt\", \"wb\") as f:\n",
    "    pk.dump(descDict, f)\n",
    "with open(r\"indexToCourse.txt\", \"wb\") as f:\n",
    "    pk.dump(indexCourse, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordIndex contains all distinct words as keys and their unique index as value\n",
    "#indexWord is the exact opposite. \n",
    "wordIndex = dict() \n",
    "index = 0\n",
    "for i in descDict:\n",
    "    for word in descDict[i][2]:\n",
    "        if word not in wordIndex.keys():\n",
    "            wordIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexWord = dict((v, k) for k, v in wordIndex.items())\n",
    "assert(len(indexWord) == len(wordIndex))\n",
    "with open(r\"indexToWord\", \"wb\") as f:\n",
    "    pk.dump(indexWord, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15248, 854)\n"
     ]
    }
   ],
   "source": [
    "occValues = []\n",
    "occRow = [] #indices of words\n",
    "occCol = [] #indices of courses\n",
    "i = 0\n",
    "for cid in descDict:\n",
    "    cIndex = descDict[cid][0]\n",
    "    for word in descDict[cid][2]:\n",
    "        occCol.append(cIndex)\n",
    "        occRow.append(wordIndex[word])\n",
    "        occValues.append(1)\n",
    "occurenceMatrix = csr_matrix((occValues, (occRow, occCol)), shape=((len(wordIndex), len(descDict))), dtype=np.int8)\n",
    "print(occurenceMatrix.shape)\n",
    "np.save(\"occ_matrix\",occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 32)\t1\n",
      "  (0, 141)\t1\n",
      "  (0, 155)\t1\n",
      "  (0, 161)\t2\n",
      "  (0, 207)\t1\n",
      "  (0, 363)\t2\n",
      "  (0, 386)\t1\n",
      "  (0, 423)\t1\n",
      "  (0, 481)\t1\n",
      "  (0, 562)\t1\n",
      "  (0, 571)\t2\n",
      "  (0, 714)\t1\n",
      "  (0, 760)\t1\n",
      "  (0, 779)\t1\n",
      "  (0, 817)\t2\n",
      "  (0, 835)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t2\n",
      "  (1, 15)\t2\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 21)\t2\n",
      "  :\t:\n",
      "  (15223, 708)\t1\n",
      "  (15224, 708)\t1\n",
      "  (15225, 708)\t1\n",
      "  (15226, 708)\t1\n",
      "  (15227, 708)\t1\n",
      "  (15228, 708)\t1\n",
      "  (15229, 708)\t1\n",
      "  (15230, 708)\t1\n",
      "  (15231, 547)\t1\n",
      "  (15232, 547)\t1\n",
      "  (15233, 547)\t1\n",
      "  (15234, 547)\t1\n",
      "  (15235, 547)\t1\n",
      "  (15236, 547)\t1\n",
      "  (15237, 547)\t1\n",
      "  (15238, 547)\t1\n",
      "  (15239, 547)\t1\n",
      "  (15240, 547)\t1\n",
      "  (15241, 547)\t1\n",
      "  (15242, 547)\t1\n",
      "  (15243, 547)\t1\n",
      "  (15244, 547)\t1\n",
      "  (15245, 547)\t1\n",
      "  (15246, 547)\t1\n",
      "  (15247, 547)\t1\n"
     ]
    }
   ],
   "source": [
    "file = np.load(\"occ_matrix.npy\")\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ixIndex = descDict['COM-308'][0]\n",
    "# ixLine = occurenceMatrix[ixIndex]\n",
    "# ixWordsFreq = dict() #for each word, (TF, DF, score)\n",
    "# for ixWord in descDict['COM-308'][2]:\n",
    "#     wordRow = wordIndex[ixWord]\n",
    "#     freq = occurenceMatrix[wordRow]\n",
    "#     ixWordsFreq[ixWord] = (occurenceMatrix[wordRow,ixIndex], freq)\n",
    "    \n",
    "# print(ixWordsFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
