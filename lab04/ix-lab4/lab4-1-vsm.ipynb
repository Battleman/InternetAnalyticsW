{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from lab04_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickleDump(filename, value):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pk.dump(value, f)\n",
    "        \n",
    "def listPrettyPrint(l):\n",
    "    \n",
    "    for a,b,c,d in zip(l[::4],l[1::4],l[2::4],l[3::4]):\n",
    "        print('{:<30}{:<30}{:<30}{:<}'.format(a,b,c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeStopWords(listWords):\n",
    "    \"\"\"\n",
    "    Filters out stopwords in a list of words\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x : len(x)>0 and x not in stopwords, listWords))\n",
    "\n",
    "def toWordList(description):\n",
    "    \"\"\"\n",
    "    takes a description (unique string) and separates it to lowercase words (not distincts) \n",
    "    \"\"\"\n",
    "    return description.lower().split(\" \") \n",
    "\n",
    "def takeOutNumbers(listWords):\n",
    "    \"\"\"\n",
    "    Takes a list of words and removes all numbers that are alone or a only seperated by h.\n",
    "    Permits to keep words that exist with a number (ex: 3SAT)\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"\\d{1,2}h\\d{0,2}$\") #removes hours\n",
    "    noHours = [pattern.sub(\"\", i) for i in listWords] \n",
    "    return list(filter(lambda x : not x.isdecimal(), noHours)) #filters out numbers-only\n",
    "\n",
    "def splitAppendedWords(descString):\n",
    "    \"\"\"\n",
    "    Takes a description, and splits appended words (because of a missing \\\\n)\n",
    "    \"\"\"\n",
    "    patternAppended = re.compile(\"([a-z][a-z])([A-Z])([a-z][a-z])\") #regex used to split bonded words \n",
    "    return patternAppended.sub(\"\\\\1 \\\\2\\\\3\", descString)\n",
    "\n",
    "def removePunctuation(descString):\n",
    "    \"\"\"\n",
    "    Removes punctuation signs in a long unique string. Treats dashes smartly.\n",
    "    \"\"\"\n",
    "    punct = \",.!?+\\n\\t:;0'%&\\\"#/()[]`\\xa0\\xad\" #list of characters that always need to be removed\n",
    "    puncttrans = str.maketrans(punct,\" \"*len(punct)) #translation rule : replace above char by a space\n",
    "    patternDash = re.compile(\" +- *| *- +\") # regex used to treat dashes\n",
    "    \n",
    "    unDashed = patternDash.sub(\"\", descString) \n",
    "    return unDashed.translate(puncttrans)\n",
    "\n",
    "\n",
    "\n",
    "def cleaner(oneCourse):\n",
    "    \"\"\"\n",
    "    Calls all above functions. First remove punctuation, then un-append words, split to space, then remove numbers\n",
    "    and stopwords. \n",
    "    \"\"\"\n",
    "    description = oneCourse['description']\n",
    "    noPunct = removePunctuation(description) #desc without punctuation\n",
    "    unAppended = splitAppendedWords(noPunct)  #desc with split words\n",
    "    \n",
    "    wordlist = toWordList(unAppended)\n",
    "    return removeStopWords(takeOutNumbers(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "explain why those functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of a dictionary that contains :\n",
    "#courses ID as keys\n",
    "#a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "descDict = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for i in courses:\n",
    "    if i['courseId'] not in descDict.keys():\n",
    "        descDict[i['courseId']] = (index, i['name'], cleaner(i))\n",
    "        indexCourse[index] = i['courseId']\n",
    "        index += 1\n",
    "\n",
    "pickleDump(r\"cidWithBag.txt\", descDict)\n",
    "pickleDump(r\"indexToCourse.txt\", indexCourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixWords = sorted(descDict['COM-308'][2])\n",
    "print(\"Words for Internet Analytics course are (in alphabetical order) :\")\n",
    "listPrettyPrint(ixWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordIndex contains all distinct words as keys and their unique index as value\n",
    "#indexWord is the exact opposite. \n",
    "wordIndex = dict() \n",
    "index = 0\n",
    "for i in descDict:\n",
    "    for word in descDict[i][2]:\n",
    "        if word not in wordIndex.keys():\n",
    "            wordIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexWord = dict((v, k) for k, v in wordIndex.items())\n",
    "assert(len(indexWord) == len(wordIndex))\n",
    "pickleDump(\"indexToWord\", indexWord)\n",
    "pickleDump(\"wordToIndex\", wordIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of sparse occurence matrix. we define values in occValues and its indices in occRow and occCol. \n",
    "#If two pairs of indices are identical, their values will be added. \n",
    "occValues = []\n",
    "occRow = [] #indices of words\n",
    "occCol = [] #indices of courses\n",
    "\n",
    "i = 0\n",
    "for cid in descDict: #iterate through all courses (and their bag of words)\n",
    "    cIndex = descDict[cid][0] #get column for this course\n",
    "    for word in descDict[cid][2]: #then append to correct list :\n",
    "        occCol.append(cIndex) #the col index\n",
    "        occRow.append(wordIndex[word]) #row index\n",
    "        occValues.append(1) #value (1, as each word represents 1 occurence)\n",
    "\n",
    "occurenceMatrix = csr_matrix((occValues, (occRow, occCol)), shape=((len(wordIndex), len(descDict))), dtype=np.float64)\n",
    "save_sparse_csr(\"occ_matrix\", occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of TF, IDF and TFIDF matrices\n",
    "mostFreqWord = occurenceMatrix.max(axis=0).data #create list of max occurence for each course\n",
    "\n",
    "#by definition : TF is term freq divided by freq of most freq word in the same document\n",
    "TF = csr_matrix(occurenceMatrix/mostFreqWord)\n",
    "\n",
    "assert(TF.max() <= 1)\n",
    "\n",
    "IDF = csr_matrix(-np.log2((occurenceMatrix != 0).sum(1)/len(descDict)))\n",
    "\n",
    "TFIDF = TF.multiply(IDF)\n",
    "np.save(\"TFIDF\", TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixIndex = descDict['COM-308'][0]\n",
    "print(sorted(find(TFIDF.getcol(ixIndex))[-1])[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
