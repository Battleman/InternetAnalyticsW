{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string #string operations\n",
    "import re #useful for regular expressions\n",
    "import nltk # to have the lemmatizer and stemmer\n",
    "import time #measure time between some operations, to have a metric\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from utils import *\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#from __future__ import print_function\n",
    "\n",
    "from lab04_helper import *\n",
    "\n",
    "allCourses = load_json('data/courses.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To have a complete list of punctions, please see the *lab_04_helper.py* file.\n",
    "To avoid errors, we preferred to have the same preprocessing function available for the notebook 2 (LSI), in order to perform term searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the following functions :\n",
    "   * **Stopwords** : Obviously, terms as \"*the*\", \"*it*\" and such do not need to appear in our bag of words. They will only cause useless noise, as they appear a lot but in (almost) every description.\n",
    "   * **Taking numbers out** : this was a decision we made. A lot of numbers are useless (such as describing time) but some are far from useless (e.g. \"*3SAT*\"). Therefore, we decided to remove lone numbers or when separated by the character *h*. This already removes most of the noise.\n",
    "   * **Split appended** : Not quite a word-filtering function but still preprocesses. Due to the scraping method used, words ending and beginning a line (originally separated by a *\\n*) were stuck together. Our function separates these, based on the presence of an uppercase charactes in the middle of a word. Sentences starting with a lower case character (thus creating fused words with only lower cases) could not be split.\n",
    "   * **Punctuation** : \"*word*\", \"*word!*\" and \"*word.*\" should be treated equally ; furthermore, punctuation signs standing on their own (not appended to a word) will create a unique word, which should not be the case. Thus, we removed all punctuation sign (see complete list in the *helper* file)\n",
    "   * **Lower cased** : our system should not be case-sensitive, so once capital letters are not important anymore (as in splitting), we put all letters to lower case. \n",
    "   \n",
    "These functions are called in a careful order to produce our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working with a total of 854 courses\n"
     ]
    }
   ],
   "source": [
    "#Creation of a dictionary corpusDict that contains :\n",
    "# - courses ID as keys\n",
    "# - a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "#indexCourse has indices as keys that link to their course (bijection mapping)\n",
    "ref_corpus = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for c in allCourses:\n",
    "    if c['courseId'] not in ref_corpus.keys(): #avoids situation where courses are tripled\n",
    "        ref_corpus[c['courseId']] = (index, c['name'], cleaner(c['description']))\n",
    "        indexCourse[index] = c['courseId']\n",
    "        index += 1\n",
    "        \n",
    "print(\"We are working with a total of %d courses\" % len(ref_corpus))\n",
    "save_pkl(ref_corpus, r\"cidWithBag\")\n",
    "save_pkl(indexCourse, r\"indexToCourse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for Internet Analytics course are (in alphabetical order) :\n",
      "acquired                      activity                      ad                            ad\n",
      "advertisement                 algebra                       algebra                       algorithm\n",
      "algorithm                     analysis                      analytics                     analytics\n",
      "analyze                       apache                        application                   application\n",
      "assessment                    auction                       auction                       balance\n",
      "based                         based                         basic                         basic\n",
      "basic                         cathedra                      chain                         class\n",
      "class                         class                         cloud                         clustering\n",
      "clustering                    collection                    com-3                         combination\n",
      "communication                 community                     community                     computing\n",
      "computing                     concept                       concept                       concrete\n",
      "content                       course                        course                        coverage\n",
      "curated                       current                       data                          data\n",
      "data                          data                          data                          data\n",
      "datasets                      datasets                      decade                        dedicated\n",
      "designed                      detection                     detection                     develop\n",
      "dimensionality                draw                          e-commerce                    e-commerce\n",
      "effectiveness                 efficiency                    end                           exam\n",
      "expected                      explore                       explore                       explore\n",
      "explore                       explores                      field                         final\n",
      "foundational                  framework                     function                      fundamental\n",
      "good                          graph                         graph                         hadoop\n",
      "hadoop                        hands-on                      homework                      homework\n",
      "important                     information                   information                   infrastructure\n",
      "inspired                      internet                      internet                      java\n",
      "key                           keywords                      knowledge                     lab\n",
      "lab                           lab                           laboratory                    large-scale\n",
      "large-scale                   large-scale                   learning                      learning\n",
      "learning                      learning                      lecture                       lecture\n",
      "linear                        linear                        machine                       machine\n",
      "main                          map-reduce                    markov                        material\n",
      "material                      medium                        method                        method\n",
      "midterm                       mining                        mining                        mining\n",
      "model                         model                         model                         model\n",
      "model                         modeling                      modelsdata-mining             network\n",
      "networking                    networking                    networking                    number\n",
      "number                        online                        online                        online\n",
      "online                        online                        outcome                       past\n",
      "practical                     practice                      prerequisite                  problem\n",
      "problem                       project                       provide                       question\n",
      "real                          real-world                    real-world                    real-world\n",
      "real-world                    recommended                   recommender                   recommender\n",
      "reduction                     related                       required                      retrieval\n",
      "retrieval                     search                        search                        seek\n",
      "self-contained                service                       service                       service\n",
      "service                       service                       session                       session\n",
      "social                        social                        social                        social\n",
      "social                        spark                         specifically                  start\n",
      "statistic                     stochastic                    stream                        stream\n",
      "structure                     student                       student                       system\n",
      "system                        teaching                      technique                     theoretical\n",
      "theory                        topic                         topic                         typical\n",
      "ubiquitous                    user                          weekly                        work\n",
      "world\t"
     ]
    }
   ],
   "source": [
    "ixWords = sorted(ref_corpus['COM-308'][2])\n",
    "print(\"Words for Internet Analytics course are (in alphabetical order) :\")\n",
    "listPrettyPrint(ixWords, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing, we are dealing with a total of 13906 \"unique\" words\n"
     ]
    }
   ],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordToIndex contains all distinct words as keys and their unique index as value\n",
    "#indexToWord is the exact opposite. \n",
    "wordToIndex = dict() \n",
    "index = 0\n",
    "for name in ref_corpus:\n",
    "    for word in ref_corpus[name][2]:\n",
    "        if word not in wordToIndex.keys():\n",
    "            wordToIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexToWord = dict((v, k) for k, v in wordToIndex.items())\n",
    "print(\"After preprocessing, we are dealing with a total of %d \\\"unique\\\" words\" % len(indexToWord))\n",
    "save_pkl(indexToWord,r\"indexToWord\")\n",
    "save_pkl( wordToIndex,r\"wordToIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of sparse occurence matrix. we define values in occValues and its indices in occRow and occCol. \n",
    "#If two pairs of indices are identical, their values will be added. \n",
    "def findOccurenceMatrix(corpus):\n",
    "    \"\"\"\n",
    "    Takes a corpus of texts (cleaned) and outputs its occurence matrix \n",
    "    (number of times each word appears in each description of the corpus)\"\"\"\n",
    "    occValues = []\n",
    "    occRow = [] #indices of words\n",
    "    occCol = [] #indices of courses\n",
    "\n",
    "    i = 0\n",
    "    for cid in corpus: #iterate through all courses ID (and their bag of words)\n",
    "        cIndex = corpus[cid][0] #get column for this course\n",
    "        for word in corpus[cid][2]: #then append to correct list :\n",
    "            occCol.append(cIndex) #the col index\n",
    "            occRow.append(wordToIndex[word]) #row index\n",
    "            occValues.append(1) #value (1, as each word represents 1 occurence)\n",
    "    return csr_matrix((occValues, (occRow, occCol)), \n",
    "                      shape=((len(wordToIndex), len(corpus))), \n",
    "                      dtype=np.float64)\n",
    "\n",
    "def getTFIDF(corpus, occMatrix, save=False):\n",
    "    \"\"\"For a corpus of text and its occurence matrix, output the corresponding TF-IDF score matrix\n",
    "    \"\"\"\n",
    "    #for each course, keep only max value ≃ occurence of most frequent word\n",
    "    mostFreqWord = occMatrix.max(axis=0).data \n",
    "\n",
    "    #by definition : TF is term freq divided by freq of most freq word in the same document\n",
    "    TF = csr_matrix(occMatrix/mostFreqWord, dtype=np.float64)\n",
    "    IDF = csr_matrix(-np.log2((occMatrix != 0).sum(1)/len(corpus)), dtype=np.float64)\n",
    "    TFIDF = TF.multiply(IDF)\n",
    "    return TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_occurenceMatrix = findOccurenceMatrix(ref_corpus)\n",
    "ref_TFIDF = getTFIDF(ref_corpus, ref_occurenceMatrix, True)\n",
    "\n",
    "#save for use in different botebooks\n",
    "save_sparse_csr(\"TFIDF\", ref_TFIDF)\n",
    "save_sparse_csr(\"occ_matrix\", ref_occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 words with greatest scores are :\n",
      "service                       online                        real-world\n",
      "social                        mining                        explore\n",
      "networking                    e-commerce                    hadoop\n",
      "large-scale                   recommender                   auction\n",
      "ad                            internet                      retrieval\n"
     ]
    }
   ],
   "source": [
    "## some work on COM-308\n",
    "ixIndex = ref_corpus['COM-308'][0]\n",
    "ix_col = ref_TFIDF.getcol(ixIndex)\n",
    "ixBigIndices = np.argsort(ix_col.todense(), axis=0)[-15:] #yields indices of the sorted values\n",
    "\n",
    "ixBigScores = [indexToWord[int(ixBigIndices[i])] for i in range(ixBigIndices.size)]\n",
    "ixBigScores.reverse()\n",
    "print(\"15 words with greatest scores are :\")\n",
    "listPrettyPrint(ixBigScores, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "The difference between high and big scores is the essence of TF-IDF : high scores indicate the term is very frequent in the document but appears in few documents, when a low score shows the word is rare in the document (appears only once or twice), but most documents have this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We create a \"new\" document with only the words we want, as a reference vector\n",
    "def sim(vec1, vec2):\n",
    "    \"\"\"Compute similarity (cosine of angle) between 2 vectors of same size\"\"\"\n",
    "    assert(len(vec1) == len(vec2))\n",
    "    prod_norm = norm(vec1)*norm(vec2)\n",
    "    prod_elem = np.asscalar(vec1.T*vec2)\n",
    "    return prod_elem/prod_norm\n",
    "\n",
    "def find5closest(query):\n",
    "    \"\"\"Finds the 5 documents with smallest angle (cosine closest to 1)\n",
    "    \n",
    "    For that, a copy of the reference corpus is made (to avoid the query affecting future queries),\n",
    "    then a new entry is added to this temp corpus (consider query as a new document) as before, with\n",
    "    a 'cleaned' query. Then the occurence matrix and TF-IDF are computed with this new 'document'\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    query -- A string\n",
    "    \"\"\"\n",
    "    cleaned_query = cleaner(query)\n",
    "    best_matches = []\n",
    "# #     Would work but need to normalize each column found... not sure it's efficient\n",
    "#     if(len(cleaned_query) <= 1):\n",
    "#         word = cleaned_query[0]\n",
    "#         row_index = wordToIndex[word]\n",
    "#         best_matches = zip(ref_TFIDF[row_index].indices, ref_TFIDF[row_index].data)\n",
    "#     else:\n",
    "    new_corpus = ref_corpus.copy() #To not affect the ref corpus, make a temp copy\n",
    "    query_index = len(ref_corpus) #query goes at last index \n",
    "    new_corpus['query'] = (query_index, 'my_query', cleaned_query) #add an entry to the corpus\n",
    "\n",
    "    query_occ_matrix = findOccurenceMatrix(new_corpus)\n",
    "    query_TFIDF = getTFIDF(new_corpus, query_occ_matrix)\n",
    "    query_vec = query_TFIDF.getcol(query_index) # ≃ TFIDF score of the query  vector\n",
    "\n",
    "    #compare the query vector to every other vectors\n",
    "    for i in range(len(ref_corpus)):\n",
    "        simil = sim(query_vec.todense(), ref_TFIDF.getcol(i).todense())\n",
    "        if(simil > 0):\n",
    "            best_matches.append((i, simil))\n",
    "\n",
    "    sort = sorted(best_matches, key=lambda x : x[1])\n",
    "    if(len(sort) >= 5):\n",
    "        return sort[-5:]\n",
    "    else:\n",
    "        return sort\n",
    "    \n",
    "def print_closest(query, result):\n",
    "    \"\"\"Pretty printer to print queried results\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    query -- The query used to find the results (string)\n",
    "    result -- List of tuples (index, score) sorted in ascending score order\n",
    "    \"\"\"\n",
    "    print(\"The course(s) most relevant to '%s' is(are):\" % query)\n",
    "    for i in result:\n",
    "        print(\"\\t\",indexCourse[i[0]],\"(\",ref_corpus[indexCourse[i[0]]][1],\") with score\",i[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2697 s  for first query, 1.0570 s for second, 2.3267 s for both\n",
      "The course(s) most relevant to 'markov chains' is(are):\n",
      "\t EE-605 ( Statistical Sequence Processing ) with score 0.28896720042\n",
      "\t MGT-526 ( Supply chain management ) with score 0.333571913525\n",
      "\t COM-516 ( Markov chains and algorithmic applications ) with score 0.344569314973\n",
      "\t MATH-332 ( Applied stochastic processes ) with score 0.521636420899\n",
      "\t MGT-484 ( Applied probability & stochastic processes ) with score 0.546921266077\n",
      "\n",
      "\n",
      "The course(s) most relevant to 'facebook' is(are):\n",
      "\t EE-727 ( Computational Social Media ) with score 0.179888165943\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bef = time.time()\n",
    "markovClosest = find5closest(\"markov chains\")\n",
    "mid = time.time()\n",
    "facebookClosest = find5closest(\"facebook\")\n",
    "aft = time.time()\n",
    "\n",
    "print(\"%.4f s  for first query, %.4f s for second, %.4f s for both\" % (mid-bef, aft-mid, aft-bef))\n",
    "print_closest(\"markov chains\", markovClosest)\n",
    "\n",
    "print_closest(\"facebook\", facebookClosest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanations\n",
    "For markov chains, we can see 5 courses are found (actually 20). This is an expected result ; scores are quite high with such diversity, indicating the courses are tightely linked to our query.\n",
    "\n",
    "On the other hand, for facebook, only 1 course is found. This is normal, as this course (EE-727) is the only in the whole corpus to contain the word 'facebook'. Indeed, to obtain a similarity score with the query > 0, a course must contain at least one word (after 'cleaning') of the query. Here facebook being the only word, only courses with this word in them will appear."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
