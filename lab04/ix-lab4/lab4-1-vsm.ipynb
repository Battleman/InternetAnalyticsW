{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string #string operations\n",
    "import re #useful for regular expressions\n",
    "import nltk # to have the lemmatizer and stemmer\n",
    "import time #measure time between some operations, to have a metric\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#from __future__ import print_function\n",
    "\n",
    "from lab04_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allCourses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickleDump(filename, value):\n",
    "    \"\"\"Save an object (e.g. table) to a pickle file to be readable by other notebooks\"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pk.dump(value, f)\n",
    "        \n",
    "def listPrettyPrint(l, n):\n",
    "    \"\"\"Prints a list l on n columns to improve readability\"\"\"\n",
    "    if(n == 4):\n",
    "        for a,b,c,d in zip(l[::4],l[1::4],l[2::4],l[3::4]):\n",
    "            print('{:<30}{:<30}{:<30}{:<}'.format(a,b,c,d))\n",
    "    if(n == 3):\n",
    "        for a,b,c in zip(l[::3],l[1::3],l[2::3]):\n",
    "            print('{:<30}{:<30}{:<}'.format(a,b,c))\n",
    "    if(len(l)%n != 0): #print remaining\n",
    "        for i in range(len(l)%n):\n",
    "            print(l[-(len(l)%n):][i], end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To have a complete list of punctions, please see the *lab_04_helper.py* file.\n",
    "To avoid errors, we preferred to have the same preprocessing function available for the notebook 2 (LSI), in order to perform term searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the following functions :\n",
    "   * **Stopwords** : Obviously, terms as \"*the*\", \"*it*\" and such do not need to appear in our bag of words. They will only cause useless noise, as they appear a lot but in (almost) every description.\n",
    "   * **Taking numbers out** : this was a decision we made. A lot of numbers are useless (such as describing time) but some are far from useless (e.g. \"*3SAT*\"). Therefore, we decided to remove lone numbers or when separated by the character *h*. This already removes most of the noise.\n",
    "   * **Split appended** : Not quite a word-filtering function but still preprocesses. Due to the scraping method used, words ending and beginning a line (originally separated by a *\\n*) were stuck together. Our function separates these, based on the presence of an uppercase charactes in the middle of a word. Sentences starting with a lower case character (thus creating fused words with only lower cases) could not be split.\n",
    "   * **Punctuation** : \"*word*\", \"*word!*\" and \"*word.*\" should be treated equally ; furthermore, punctuation signs standing on their own (not appended to a word) will create a unique word, which should not be the case. Thus, we removed all punctuation sign (see complete list in the *helper* file)\n",
    "   * **Lower cased** : our system should not be case-sensitive, so once capital letters are not important anymore (as in splitting), we put all letters to lower case. \n",
    "   \n",
    "These functions are called in a careful order to produce our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working with a total of 854 courses\n"
     ]
    }
   ],
   "source": [
    "#Creation of a dictionary corpusDict that contains :\n",
    "# - courses ID as keys\n",
    "# - a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "#indexCourse has indices as keys that link to their course (bijection mapping)\n",
    "ref_corpus = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for c in allCourses:\n",
    "    if c['courseId'] not in ref_corpus.keys(): #avoids situation where courses are tripled\n",
    "        ref_corpus[c['courseId']] = (index, c['name'], cleaner(c['description']))\n",
    "        indexCourse[index] = c['courseId']\n",
    "        index += 1\n",
    "        \n",
    "print(\"We are working with a total of %d courses\" % len(ref_corpus))\n",
    "pickleDump(r\"cidWithBag.txt\", ref_corpus)\n",
    "pickleDump(r\"indexToCourse.txt\", indexCourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for Internet Analytics course are (in alphabetical order) :\n",
      "acquired                      activities                    ad                            ad\n",
      "advertisement                 algebra                       algebra                       algorithms\n",
      "algorithms                    analysis                      analytics                     analytics\n",
      "analyze                       apache                        applications                  applications\n",
      "assessment                    auctions                      auctions                      balance\n",
      "based                         based                         basic                         basic\n",
      "basic                         cathedra                      chains                        class\n",
      "class                         class                         cloud                         clustering\n",
      "clustering                    collection                    com-3                         combination\n",
      "communication                 community                     community                     computing\n",
      "computing                     concepts                      concepts                      concrete\n",
      "content                       courses                       courses                       coverage\n",
      "curated                       current                       data                          data\n",
      "data                          data                          data                          data\n",
      "datasets                      datasets                      decade                        dedicated\n",
      "designed                      detection                     detection                     develop\n",
      "dimensionality                draw                          e-commerce                    e-commerce\n",
      "effectiveness                 efficiency                    end                           exam\n",
      "expected                      explore                       explore                       explore\n",
      "explore                       explores                      fields                        final\n",
      "foundational                  frameworks                    functions                     fundamental\n",
      "good                          graph                         graphs                        hadoop\n",
      "hadoop                        hands-on                      homeworks                     homeworks\n",
      "important                     information                   information                   infrastructure\n",
      "inspired                      internet                      internet                      java\n",
      "key                           keywords                      knowledge                     lab\n",
      "laboratory                    labs                          labs                          large-scale\n",
      "large-scale                   large-scale                   learning                      learning\n",
      "learning                      learning                      lectures                      lectures\n",
      "linear                        linear                        machine                       machine\n",
      "main                          map-reduce                    markov                        material\n",
      "materials                     media                         methods                       methods\n",
      "midterm                       mining                        mining                        mining\n",
      "modeling                      models                        models                        models\n",
      "models                        models                        modelsdata-mining             networking\n",
      "networking                    networking                    networks                      number\n",
      "number                        online                        online                        online\n",
      "online                        online                        outcomes                      past\n",
      "practical                     practice                      prerequisites                 problems\n",
      "problems                      project                       provide                       questions\n",
      "real                          real-world                    real-world                    real-world\n",
      "real-world                    recommended                   recommender                   recommender\n",
      "reduction                     related                       required                      retrieval\n",
      "retrieval                     search                        search                        seeks\n",
      "self-contained                services                      services                      services\n",
      "services                      services                      sessions                      sessions\n",
      "social                        social                        social                        social\n",
      "social                        spark                         specifically                  start\n",
      "statistics                    stochastic                    stream                        stream\n",
      "structures                    student                       student                       systems\n",
      "systems                       teaching                      techniques                    theoretical\n",
      "theory                        topic                         topic                         typical\n",
      "ubiquitous                    user                          weekly                        work\n",
      "world\t"
     ]
    }
   ],
   "source": [
    "ixWords = sorted(ref_corpus['COM-308'][2])\n",
    "print(\"Words for Internet Analytics course are (in alphabetical order) :\")\n",
    "listPrettyPrint(ixWords, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing, we are dealing with a total of 15248 \"unique\" words\n"
     ]
    }
   ],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordToIndex contains all distinct words as keys and their unique index as value\n",
    "#indexToWord is the exact opposite. \n",
    "wordToIndex = dict() \n",
    "index = 0\n",
    "for name in ref_corpus:\n",
    "    for word in ref_corpus[name][2]:\n",
    "        if word not in wordToIndex.keys():\n",
    "            wordToIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexToWord = dict((v, k) for k, v in wordToIndex.items())\n",
    "print(\"After preprocessing, we are dealing with a total of %d \\\"unique\\\" words\" % len(indexToWord))\n",
    "pickleDump(\"indexToWord\", indexToWord)\n",
    "pickleDump(\"wordToIndex\", wordToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of sparse occurence matrix. we define values in occValues and its indices in occRow and occCol. \n",
    "#If two pairs of indices are identical, their values will be added. \n",
    "def findOccurenceMatrix(corpus):\n",
    "    \"\"\"\n",
    "    Takes a corpus of texts (cleaned) and outputs its occurence matrix \n",
    "    (number of times each word appears in each description of the corpus)\"\"\"\n",
    "    occValues = []\n",
    "    occRow = [] #indices of words\n",
    "    occCol = [] #indices of courses\n",
    "\n",
    "    i = 0\n",
    "    for cid in corpus: #iterate through all courses ID (and their bag of words)\n",
    "        cIndex = corpus[cid][0] #get column for this course\n",
    "        for word in corpus[cid][2]: #then append to correct list :\n",
    "            occCol.append(cIndex) #the col index\n",
    "            occRow.append(wordToIndex[word]) #row index\n",
    "            occValues.append(1) #value (1, as each word represents 1 occurence)\n",
    "    return csr_matrix((occValues, (occRow, occCol)), \n",
    "                      shape=((len(wordToIndex), len(corpus))), \n",
    "                      dtype=np.float64)\n",
    "\n",
    "def findTFIDF(corpus, occMatrix):\n",
    "    \"\"\"For a corpus of text and its occurence matrix, output the corresponding TF-IDF score matrix\n",
    "    \"\"\"\n",
    "    #for each course, keep only max value â‰ƒ occurence of most frequent word\n",
    "    mostFreqWord = occMatrix.max(axis=0).data \n",
    "\n",
    "    #by definition : TF is term freq divided by freq of most freq word in the same document\n",
    "    TF = csr_matrix(occMatrix/mostFreqWord)\n",
    "    IDF = csr_matrix(-np.log2((occMatrix != 0).sum(1)/len(corpus)))\n",
    "    return TF.multiply(IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_occurenceMatrix = findOccurenceMatrix(corpus)\n",
    "ref_TFIDF = findTFIDF(corpus, ref_occurenceMatrix)\n",
    "\n",
    "#save for use in different botebooks\n",
    "np.save(\"TFIDF\", ref_TFIDF)\n",
    "save_sparse_csr(\"occ_matrix\", ref_occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with greatest scores are :\n",
      "services                      online                        real-world\n",
      "social                        mining                        explore\n",
      "networking                    e-commerce                    hadoop\n",
      "large-scale                   recommender                   ad\n",
      "auctions                      retrieval                     internet\n"
     ]
    }
   ],
   "source": [
    "## some work on COM-308\n",
    "ixIndex = corpus['COM-308'][0]\n",
    "tmp = ref_TFIDF.getcol(ixIndex)\n",
    "ixBigIndices = (np.argsort(tmp.todense(), axis=0)[-15:])\n",
    "\n",
    "ixBigScores = []\n",
    "for i in ixBigIndices:\n",
    "    ixBigScores.append(indexToWord[int(i)])\n",
    "ixBigScores.reverse()\n",
    "print(\"15 words with greatest scores are :\")\n",
    "listPrettyPrint(ixBigScores, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "The difference between high and big scores is the essence of TF-IDF : high scores indicate the term is very frequent in the document but appears in few documents, when a low score shows the word is rare in the document (appears only once or twice), but most documents have this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The courses most relevant to 'markov chains' are:\n",
      "\t MGT-602 ( Mathematical models in supply chain management ) with score 0.135284122136\n",
      "\t EE-605 ( Statistical Sequence Processing ) with score 0.247027318939\n",
      "\t COM-516 ( Markov chains and algorithmic applications ) with score 0.279644274\n",
      "\t MATH-332 ( Applied stochastic processes ) with score 0.377670348156\n",
      "\t MGT-484 ( Applied probability & stochastic processes ) with score 0.513049726123\n",
      "The courses most relevant to 'facebook' are:\n",
      "\t EE-727 ( Computational Social Media ) with score 0.175401460645\n"
     ]
    }
   ],
   "source": [
    "## We create a \"new\" document with only the words we want, as a reference vector\n",
    "def sim(vec1, vec2):\n",
    "    assert(len(vec1) == len(vec2))\n",
    "    prod_norm = norm(vec1)*norm(vec2)\n",
    "    prod_elem = np.asscalar(vec1.T*vec2)\n",
    "    return prod_elem/prod_norm\n",
    "\n",
    "def find5closest(query):\n",
    "    \"\"\"Finds the 5 documents with smallest angle (cosine closest to 1)\"\"\"\n",
    "    new_corpus = ref_corpus.copy()\n",
    "    query_index = len(ref_corpus)\n",
    "    new_corpus['query'] = (query_index, 'my_query', cleaner(query))\n",
    "    query_occ_matrix = findOccurenceMatrix(new_corpus)\n",
    "    query_TFIDF = findTFIDF(new_corpus, query_occ_matrix)\n",
    "    query_vec = query_TFIDF.getcol(query_index)\n",
    "    hall_of_fame = []\n",
    "    for i in range(len(descDict)):\n",
    "        simil = sim(query_vec.todense(), ref_TFIDF.getcol(i).todense())\n",
    "        if(simil > 0):\n",
    "            hall_of_fame.append((i, simil))\n",
    "    sort = sorted(hall_of_fame, key=lambda x : x[1])\n",
    "    if(len(sort) >= 5):\n",
    "        return sort[-5:]\n",
    "    else:\n",
    "        return sort\n",
    "    \n",
    "def print_closest(query, result):\n",
    "    print(\"The courses most relevant to '%s' are:\" % query)\n",
    "    for i in result:\n",
    "        print(\"\\t\",indexCourse[i[0]],\"(\",descDict[indexCourse[i[0]]][1],\") with score\",i[1])\n",
    "markovClosest = find5closest(\"markov chains\")\n",
    "facebookClosest = find5closest(\"facebook\")\n",
    "\n",
    "print_closest(\"markov chains\", markovClosest)\n",
    "print_closest(\"facebook\", facebookClosest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanations\n",
    "For markov chains, we can see 5 courses are found (actually 20). This is an expected result ; scores are quite high with such diversity, indicating the courses are tightely linked to our query.\n",
    "\n",
    "On the other hand, for facebook, only 1 course is found. This is normal, as this course (EE-727) is the only in the whole corpus to contain the word 'facebook'. Indeed, to obtain a similarity score with the query > 0, a course must contain at least one word (after 'cleaning') of the query. Here facebook being the only word, only courses with this word in them will appear."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
