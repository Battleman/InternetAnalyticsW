{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from lab04_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickleDump(filename, value):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pk.dump(value, f)\n",
    "        \n",
    "def listPrettyPrint(l):\n",
    "    \n",
    "    for a,b,c,d in zip(l[::4],l[1::4],l[2::4],l[3::4]):\n",
    "        print('{:<30}{:<30}{:<30}{:<}'.format(a,b,c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "See lab_04 helper.\n",
    "To avoid error we prefered to be able to have the preprocessing function also available for the part 2 : lsi to perfor term search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "explain why those functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of a dictionary that contains :\n",
    "#courses ID as keys\n",
    "#a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "descDict = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for i in courses:\n",
    "    if i['courseId'] not in descDict.keys():\n",
    "        descDict[i['courseId']] = (index, i['name'], cleaner(i))\n",
    "        indexCourse[index] = i['courseId']\n",
    "        index += 1\n",
    "\n",
    "pickleDump(r\"cidWithBag.txt\", descDict)\n",
    "pickleDump(r\"indexToCourse.txt\", indexCourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixWords = sorted(descDict['COM-308'][2])\n",
    "print(\"Words for Internet Analytics course are (in alphabetical order) :\")\n",
    "listPrettyPrint(ixWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordIndex contains all distinct words as keys and their unique index as value\n",
    "#indexWord is the exact opposite. \n",
    "wordIndex = dict() \n",
    "index = 0\n",
    "for i in descDict:\n",
    "    for word in descDict[i][2]:\n",
    "        if word not in wordIndex.keys():\n",
    "            wordIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexWord = dict((v, k) for k, v in wordIndex.items())\n",
    "assert(len(indexWord) == len(wordIndex))\n",
    "with open(r\"indexToWord\", \"wb\") as f:\n",
    "    pk.dump(indexWord, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occValues = []\n",
    "occRow = [] #indices of words\n",
    "occCol = [] #indices of courses\n",
    "i = 0\n",
    "for cid in descDict:\n",
    "    cIndex = descDict[cid][0]\n",
    "    for word in descDict[cid][2]:\n",
    "        occCol.append(cIndex)\n",
    "        occRow.append(wordIndex[word])\n",
    "        occValues.append(1)\n",
    "occurenceMatrix = csr_matrix((occValues, (occRow, occCol)), shape=((len(wordIndex), len(descDict))), dtype=np.float64)\n",
    "save_sparse_csr(\"occ_matrix\", occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostFreqWord = occurenceMatrix.max(axis=0).data\n",
    "TF = occurenceMatrix/mostFreqWord\n",
    "IDF = -np.log((occurenceMatrix != 0).sum(1)/len(descDict))\n",
    "\n",
    "print(TF.shape, IDF.shape)\n",
    "TFIDF = np.multiply(TF, IDF)\n",
    "\n",
    "print(TFIDF.shape)\n",
    "np.save(\"TFIDF\", TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ixIndex = descDict['COM-308'][0]\n",
    "# ixLine = occurenceMatrix[ixIndex]\n",
    "# ixWordsFreq = dict() #for each word, (TF, DF, score)\n",
    "# for ixWord in descDict['COM-308'][2]:\n",
    "#     wordRow = wordIndex[ixWord]\n",
    "#     freq = occurenceMatrix[wordRow]\n",
    "#     ixWordsFreq[ixWord] = (occurenceMatrix[wordRow,ixIndex], freq)\n",
    "    \n",
    "# print(ixWordsFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
