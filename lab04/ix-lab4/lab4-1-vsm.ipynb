{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import string #string operations\n",
    "import re #useful for regular expressions\n",
    "import nltk # to have the lemmatizer and stemmer\n",
    "#import time #measure time between some operations, to have a metric\n",
    "\n",
    "from scipy.sparse import csr_matrix, find\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from lab04_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickleDump(filename, value):\n",
    "    \"\"\"Save an object (e.g. table) to a pickle file to be readable by other notebooks\"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pk.dump(value, f)\n",
    "        \n",
    "def listPrettyPrint(l, n):\n",
    "    \"\"\"Prints a list l on n columns to improve readability\"\"\"\n",
    "    if(n == 4):\n",
    "        for a,b,c,d in zip(l[::4],l[1::4],l[2::4],l[3::4]):\n",
    "            print('{:<30}{:<30}{:<30}{:<}'.format(a,b,c,d))\n",
    "    if(n == 3):\n",
    "        for a,b,c in zip(l[::3],l[1::3],l[2::3]):\n",
    "            print('{:<30}{:<30}{:<}'.format(a,b,c))\n",
    "    if(len(l)%n != 0):\n",
    "        for i in range(len(l)%n):\n",
    "            print(l[-(len(l)%n):][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To have a complete list of punctions, please see the *lab_04_helper.py* file.\n",
    "To avoid errors, we preferred to have the same preprocessing function available for the notebook 2 (LSI), in order to perform term searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the following functions :\n",
    "   * **Stopwords** : Obviously, terms as \"*the*\", \"*it*\" and such do not need to appear in our bag of words. They will only cause useless noise, as they appear a lot but in (almost) every description.\n",
    "   * **Taking numbers out** : this was a decision we made. A lot of numbers are useless (such as describing time) but some are far from useless (e.g. \"*3SAT*\"). Therefore, we decided to remove lone numbers or when separated by the character *h*. This already removes most of the noise.\n",
    "   * **Split appended** : Not quite a word-filtering function but still preprocesses. Due to the scraping method used, words ending and beginning a line (originally separated by a *\\n*) were stuck together. Our function separates these, based on the presence of an uppercase charactes in the middle of a word. Sentences starting with a lower case character (thus creating fused words with only lower cases) could not be split.\n",
    "   * **Punctuation** : \"*word*\", \"*word!*\" and \"*word.*\" should be treated equally ; furthermore, punctuation signs standing on their own (not appended to a word) will create a unique word, which should not be the case. Thus, we removed all punctuation sign (see complete list in the *helper* file)\n",
    "   * **Lower cased** : our system should not be case-sensitive, so once capital letters are not important anymore (as in splitting), we put all letters to lower case. \n",
    "   \n",
    "These functions are called in a careful order to produce our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of a dictionary descDIct that contains :\n",
    "# - courses ID as keys\n",
    "# - a 3-tuple(uniqueIndex, title, list[separated words]) as value\n",
    "#indexCourse has indices as keys that link to their course (bijection mapping)\n",
    "descDict = dict() \n",
    "indexCourse = dict()\n",
    "index = 0\n",
    "for c in courses:\n",
    "    if c['courseId'] not in descDict.keys(): #avoids situation where courses are tripled\n",
    "        descDict[c['courseId']] = (index, c['name'], cleaner(c['description']))\n",
    "        indexCourse[index] = c['courseId']\n",
    "        index += 1\n",
    "        \n",
    "print(\"We are working with a total of %d courses\" % len(descDict))\n",
    "pickleDump(r\"cidWithBag.txt\", descDict)\n",
    "pickleDump(r\"indexToCourse.txt\", indexCourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixWords = sorted(descDict['COM-308'][2])\n",
    "print(\"Words for Internet Analytics course are (in alphabetical order) :\")\n",
    "listPrettyPrint(ixWords, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creation of 2 dictionary.\n",
    "#wordIndex contains all distinct words as keys and their unique index as value\n",
    "#indexWord is the exact opposite. \n",
    "wordIndex = dict() \n",
    "index = 0\n",
    "for i in descDict:\n",
    "    for word in descDict[i][2]:\n",
    "        if word not in wordIndex.keys():\n",
    "            wordIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "indexWord = dict((v, k) for k, v in wordIndex.items())\n",
    "assert(len(indexWord) == len(wordIndex))\n",
    "print(\"After preprocessing, we are dealing with a total of %d \\\"unique\\\" words\" % len(indexWord))\n",
    "pickleDump(\"indexToWord\", indexWord)\n",
    "pickleDump(\"wordToIndex\", wordIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of sparse occurence matrix. we define values in occValues and its indices in occRow and occCol. \n",
    "#If two pairs of indices are identical, their values will be added. \n",
    "occValues = []\n",
    "occRow = [] #indices of words\n",
    "occCol = [] #indices of courses\n",
    "\n",
    "i = 0\n",
    "for cid in descDict: #iterate through all courses ID (and their bag of words)\n",
    "    cIndex = descDict[cid][0] #get column for this course\n",
    "    for word in descDict[cid][2]: #then append to correct list :\n",
    "        occCol.append(cIndex) #the col index\n",
    "        occRow.append(wordIndex[word]) #row index\n",
    "        occValues.append(1) #value (1, as each word represents 1 occurence)\n",
    "\n",
    "occurenceMatrix = csr_matrix((occValues, (occRow, occCol)), shape=((len(wordIndex), len(descDict))), dtype=np.float64)\n",
    "save_sparse_csr(\"occ_matrix\", occurenceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of TF, IDF and TFIDF matrices\n",
    "mostFreqWord = occurenceMatrix.max(axis=0).data #create list of max occurence for each course\n",
    "\n",
    "#by definition : TF is term freq divided by freq of most freq word in the same document\n",
    "TF = csr_matrix(occurenceMatrix/mostFreqWord)\n",
    "IDF = csr_matrix(-np.log2((occurenceMatrix != 0).sum(1)/len(descDict)))\n",
    "\n",
    "TFIDF = TF.multiply(IDF)\n",
    "np.save(\"TFIDF\", TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixIndex = descDict['COM-308'][0]\n",
    "tmp = TFIDF.getcol(ixIndex)\n",
    "ixBigIndices = (np.argsort(tmp.todense(), axis=0)[-15:])\n",
    "\n",
    "\n",
    "ixBigScores = []\n",
    "for i in ixBigIndices:\n",
    "    ixBigScores.append(indexWord[int(i)])\n",
    "ixBigScores.reverse()\n",
    "print(\"Words with greatest scores are :\")\n",
    "listPrettyPrint(ixBigScores, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "The difference between high and big scores is the essence of TF-IDF : high scores indicate the term is very frequent in the document but appears in few documents, when a low score shows the word is rare in the document (appears only once or twice), but most documents have this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
