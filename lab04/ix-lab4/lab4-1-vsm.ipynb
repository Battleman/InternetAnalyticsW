{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *W*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Cloux Olivier*\n",
    "* *Reiss Saskia*\n",
    "* *Urien Thibault*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses = load_json('data/courses.txt') \n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeStopWords(listWords):\n",
    "    \"\"\"\n",
    "    Filters out stopwords in a list of words\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x : len(x)>0 and x not in stopwords, listWords))\n",
    "\n",
    "def toWordList(description):\n",
    "    \"\"\"\n",
    "    - Removes punctuation but treats smartly words with dash (e.g. keeps dash in 'k-mean')\n",
    "    - Some words were bonded together because of missing newline (yielding words as 'wordAnotherword'). We split those.\n",
    "    - Puts all words in lowercase.\n",
    "    - Returns a list of cleaned words. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    description -- A unique, long string\n",
    "    \"\"\"\n",
    "    return description.lower().split(\" \") \n",
    "\n",
    "def takeOutNumbers(listWords):\n",
    "    \"\"\"\n",
    "    Removes all numbers that are alone or a only seperated by h.\n",
    "    Permits to keep words that exist with a number (ex: 3SAT)\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"\\d{1,2}h\\d{0,2}$\")\n",
    "    noHours = [pattern.sub(\"\", i) for i in listWords]\n",
    "    return list(filter(lambda x : not x.isdecimal(), noHours))\n",
    "\n",
    "def splitAppendedWords(descString):\n",
    "    patternAppended = re.compile(\"([a-z][a-z])([A-Z])([a-z][a-z])\") #regex used to split bonded words \n",
    "    return patternAppended.sub(\"\\\\1 \\\\2\\\\3\", descString)\n",
    "\n",
    "def removePunctuation(descString):\n",
    "    punct = \",.!?+\\n\\t:;0'%&\\\"#/()[]`\\xa0\" #list of characters that always need to be removed\n",
    "    puncttrans = str.maketrans(punct,\" \"*len(punct)) #translation rule : replace above char by a space\n",
    "    patternDash = re.compile(\"\\ +-\\ *|\\ *-\\ +\") # regex used to treat dashes\n",
    "    \n",
    "    unDashed = patternDash.sub(\"\", descString) \n",
    "    return unDashed.translate(puncttrans)\n",
    "\n",
    "\n",
    "\n",
    "def cleaner(oneCourse):\n",
    "    description = oneCourse['description']\n",
    "    noPunct = removePunctuation(description) #desc without punctuation\n",
    "    unAppended = splitAppendedWords(noPunct)  #desc with split words\n",
    "    \n",
    "    wordlist = toWordList(unAppended)\n",
    "    return removeStopWords(takeOutNumbers(wordlist))\n",
    "\n",
    "# def veryFrequent():\n",
    "    \n",
    "# def inFrequent():\n",
    "    \n",
    "# def stemming():\n",
    "\n",
    "# def lemmatise():\n",
    "    \n",
    "# def ngram():\n",
    "\n",
    "# def blackMagic():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descDict = dict() #will contain all courses, and a tuple(uniqueIndex, title, list[separated words])\n",
    "index = 0\n",
    "for i in courses:\n",
    "    if i['courseId'] not in descDict.keys():\n",
    "        descDict[i['courseId']] = (index, i['name'], cleaner(i))\n",
    "    #     print(i['courseId'], index)\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordIndex = dict() #will contain all distinct words and a unique index\n",
    "index = 0\n",
    "for i in descDict:\n",
    "    for word in descDict[i][2]:\n",
    "        if word not in wordIndex.keys():\n",
    "            wordIndex[word] = index\n",
    "            index += 1;\n",
    "\n",
    "#invert dict that wordIndex : each index is mapped to a distinct word\n",
    "indexWord = dict((v, k) for k, v in wordIndex.items()) \n",
    "assert(len(indexWord) == len(wordIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigBadassMatrix = np.zeros((len(wordIndex), len(descDict)))\n",
    "for courseId in descDict:\n",
    "    column = descDict[courseId][0]\n",
    "    for word in descDict[courseId][2]:\n",
    "        wid = wordIndex[word]\n",
    "        bigBadassMatrix[wid][column] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ixIndex = descDict['COM-308'][0]\n",
    "ixLine = bigBadassMatrix[ixIndex]\n",
    "ixWordsFreq = dict() #for each word, (TF, DF, score)\n",
    "for ixWord in descDict['COM-308'][2]:\n",
    "    wordRow = wordIndex[ixWord]\n",
    "    freq = sum(bigBadassMatrix[wordRow])\n",
    "    ixWordsFreq[ixWord] = (bigBadassMatrix[wordRow][ixIndex], freq)\n",
    "    \n",
    "print(ixWordsFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
