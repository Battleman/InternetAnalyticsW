{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** W\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Olivier Cloux\n",
    "* Thibault Urien\n",
    "* Saskia Reiss\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given imports\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from scipy.sparse import find\n",
    "\n",
    "# Import pickle to open processed courses\n",
    "import pickle as pk\n",
    "# Use Sparks sparse vectors\n",
    "from pyspark.mllib.linalg import Vectors \n",
    "\n",
    "from utils import load_pkl, listPrettyPrint\n",
    "from lab04_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's open the pickle to load it.\n",
    "processedCourses = load_pkl(\"cidWithBag\")\n",
    "\n",
    "# We also need the list of words\n",
    "uniqueWords = load_pkl(\"indexToWord\")\n",
    "\n",
    "# Get the course indexes for the matrix\n",
    "courseIndices = load_pkl(\"indexToCourse\")\n",
    "\n",
    "# Get the TF-IDF matrix\n",
    "TF_course_matrix = load_sparse_csr(\"TFIDF.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform the list of words into a spark vector usig TF-IDF\n",
    "#### Use snippets Get value with index from RDD and Spar's sparse vectors\n",
    "\n",
    "# Create an RDD we will fill with our sparse vectors \n",
    "v_dim = len(uniqueWords)\n",
    "vector_list = []\n",
    "\n",
    "# Create the sparse vectors using the matrix.\n",
    "for i in range (len(courseIndices)):\n",
    "    values = find(TF_course_matrix.getcol(i))[-1]\n",
    "    indices = (TF_course_matrix.getcol(i).nonzero())[0] # Could use first of find though\n",
    "    v = Vectors.sparse(v_dim, indices, values)\n",
    "    vector_list.append((i, v))\n",
    "    \n",
    "# In the RDD we need a tuple wit (index of document, corresponding SparseVector) then map(list)\n",
    "# Create the RDD (corpus) from the list\n",
    "corpus = sc.parallelize(vector_list)\n",
    "#print(corpus.take(2))\n",
    "corpus = corpus.map(list)\n",
    "\n",
    "# Cluster the documents into ten topics using LDA\n",
    "ldaModel = LDA.train(corpus, seed=2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printTopics(model, maxTerms):\n",
    "    # Gives a list of the 10 most linked words to our topics\n",
    "    topic_indices = model.describeTopics(maxTermsPerTopic=maxTerms)\n",
    "\n",
    "    # Extract the indices (first tuple) from the list of word\n",
    "    topic = 1\n",
    "    for i in topic_indices:\n",
    "        print(\"Topic #\", topic)\n",
    "        listPrettyPrint([str(\"%s - %.7f\" % (uniqueWords[i[0][j]], i[1][j]))  for j in range(maxTerms)], 3)\n",
    "        print('\\n')\n",
    "        topic += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printTopics(ldaModel, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Topic 1** : Finanancial Engineering\n",
    "* **Topic 2** : Circuits & Systems\n",
    "* **Topic 3** : Risk Managment\n",
    "* **Topic 4** : Physics (Acoustic Waves)\n",
    "* **Topic 5** : Electrical Engineering\n",
    "* **Topic 6** : Environmental Engineering\n",
    "* **Topic 7** : Seminars & Talks\n",
    "* **Topic 8** : Chemistry\n",
    "* **Topic 9** : Astrophysics\n",
    "* **Topic 10** : Bioengineering and Bology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, LDA is a bit more deterministic and precise than LSI, even if we do seem to have some words that seem to not correspond to our main topics, probably because k is too big. \n",
    "Note also that LDA is a bit more random, as it will extract the same topics each time, but their order of appearence will be random, except if we lock te seed beforehand (as we did to be sre to have the same tpic order each time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying the prior on topic distribution α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_indices_list = []\n",
    "array_a = [2, 6, 15, 25, 52]\n",
    "\n",
    "for i in array_a:\n",
    "    model = (LDA.train(corpus, k=10, seed=2, docConcentration=float(i), topicConcentration=1.01))\n",
    "    topic_indices_list.append(model.describeTopics(maxTermsPerTopic=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for topic in range(1,10):\n",
    "    print(\"\\n\\t\\tTOPIC %d beta=1.01\\n\" %(topic))\n",
    "    alpha_index = 0\n",
    "    for i in range(0, 5):\n",
    "        print(\"\\n\\t alpha=%d \\n\" %(array_a[alpha_index]))\n",
    "        j = topic_indices_list[i][topic-1]\n",
    "        listPrettyPrint([str(\"%s - %.7f\" % (uniqueWords[j[0][k]], j[1][k]))  for k in range(10)], 3)\n",
    "        alpha_index += 1\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, setting an higher alpha will make so that each document is only composed of some very broad and general topics, while a lower value will show more precise and dominant topics.\n",
    "In fact, an alpha is what we now already about the data (being a prior) and thus, we say ourselves if the tpic is very geral and broad, or if we want something pecific.\n",
    "\n",
    "Also note that between 52 and 102 (not shown here), we have the same topics. This means that we need to play with lower values of alpha to see topic changes (like we have between 25 and 52)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying the prior on word distribution per topic β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_indices_list = []\n",
    "array = [1.1, 2.0, 6.0, 20.0]\n",
    "\n",
    "for i in array:\n",
    "    model = (LDA.train(corpus, k=10, seed=2, docConcentration=6.0, topicConcentration=float(i)))\n",
    "    topic_indices_list.append(model.describeTopics(maxTermsPerTopic=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic in range(1,10):\n",
    "    print(\"\\n\\t\\tTOPIC %d alpha=6\\n\" %(topic))\n",
    "    beta_index = 0\n",
    "    for i in range(0, 4):\n",
    "        print(\"\\n\\t beta=%f \\n\" %(array[beta_index]))\n",
    "        j = topic_indices_list[i][topic-1]\n",
    "        listPrettyPrint([str(\"%s - %.7f\" % (uniqueWords[j[0][k]], j[1][k]))  for k in range(10)], 3)\n",
    "        beta_index += 1\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a low beta value places more weight on having each topic composed of only a few dominant words, while a igher beta value will have far broader terms. So alpha and beta have similr effects, even if not the same words are extracted.\n",
    "\n",
    "Also note that between 6 and 20, we have the exact same topics, so we need to take lower values for beta to have any different topics (like between 6 and 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the most value of the EPFL taught subjects, we are going to take out 7 topics, one for each faculty taught at the EPFL (not counting individual sections or we would have k = 24) so or k will be equal to 7.\n",
    "Now, we are going to choose α and β such that we use the most common values used for both of them. According to documentation, we thus have α = 50/7 = 7.14 ans β = 1.01 or 200/W with W the number of words in the vocabulary. We may use 1.01, as W can be very big and we need β to be grater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = len(uniqueWords)\n",
    "print(200/w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this value is too small, so we will use β = 1.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epfl_model = (LDA.train(corpus, k=7, seed=2, docConcentration=7.14, topicConcentration=1.01))\n",
    "printTopics(epfl_model, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the labels are quite similar form what we would think would appear in the tought subjects : we will find the 7 faculties in a form or another in there topics.\n",
    "\n",
    "So we have:\n",
    "* **Topic 1** : Administrative (most courses will have some form of adminsitration discussion in their description)\n",
    "* **Topic 2** : Cancer & Global Health + Blue Brain Project (Life Sciences)\n",
    "* **Topic 3** : Life Sciences (Bioengineering)\n",
    "* **Topic 4** : Financial Engineering (Managment of Technology)\n",
    "* **Topic 5** : Electrical and Mechanical Engineering (Engineering)\n",
    "* **Topic 6** : Architecture, Civil and Environmental Engineering\n",
    "* **Topic 7** : Basic Sciences (Chemistry, Mahematics, Physics)\n",
    "\n",
    "Note that we don't seem to have our own faculty (Computer and Communication Sciences) but this might be due to the fact that in most Bachelor years, we have courses in other domains, like physics, mathematics and electrical engineering, rather than pure computre science courses. This makes the presence of technical terms to such topics much less present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the same logic as for the EPFL's taught subjects, we are going to choose k = 16, as we can see on the wikipeia for schools website that they have 16 sections.\n",
    "\n",
    "As for α and β, we use the most common values for both of them again. Thus, we have α = 50/16 = 3.13 and β = 1.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to do TF-IDF of the wiki txt, then re-use code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
