{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2: Latent semantic indexing\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** W\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* Olivier Cloux\n",
    "* Thibault Urien\n",
    "* Saskia Reiss\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 2 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don‚Äôt forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.sparse.linalg import svds\n",
    "from lab04_helper import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Latent semantic indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 300\n",
    "td_matrix = load_sparse_csr(\"occ_matrix.npz\")\n",
    "# SVD decomposition\n",
    "print(td_matrix.shape)\n",
    "u, s, v = svds(td_matrix,k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_k = u[:,:k]\n",
    "v_k_T = v[:k,:]\n",
    "s_k = np.diag(s[:k])\n",
    "print(\"20 biggest eigenvalues : \\n\",s[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix U, each __row__ correspond to a term, and for each term the value index i correspond to how much relevant the ith concept is for this term.\n",
    "In the matrix $V^T$, each __column__ correspond to a document(cours description), and for each document the value at index i correspond to how much relevant the ith concept is for this term. The values on the diagonal of S corespound to the importance of each topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 10\n",
    "wordIndex = load_pkl(\"indexToWord\")\n",
    "coursIndex = load_pkl(\"indexToCourse\")\n",
    "\n",
    "n_topic_relevant_term_index = np.argsort(u_k , 0)[-n:,:]\n",
    "n_topic_relevant_document_index = np.argsort(v_k_T, 1)[:,-n:]\n",
    "\n",
    "allCourses = load_json('data/courses.txt') \n",
    "nameDict =  {t[\"courseId\"]:t[\"name\"] for t in allCourses}\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    print(\"\\n*************************************************\")\n",
    "    print(\"Topic #\",(i+1),\"\\n\")\n",
    "    listPrettyPrint( [\"%s - %.7f\" %(wordIndex[idx], u_k[idx,i])for idx in reversed(n_topic_relevant_term_index[-m:,i])],3)\n",
    "    print(\"\\n\\n\")\n",
    "    for s in [\"%s - %.7f\" % (nameDict[coursIndex[idx]] , v_k_T[i,idx]) for idx in reversed(n_topic_relevant_document_index[i,-m:])] :\n",
    "        print(s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "* **Topic 1** Signals and waves\n",
    "* **Topic 2** Electronic components and finance\n",
    "* **Topic 3** Applied physic mostly in life sciences\n",
    "* **Topic 4** Applied physic mostly for micro/nano crafts\n",
    "* **Topic 5** Big scale projects\n",
    "* **Topic 6** Handling chaos \n",
    "* **Topic 7** Thing that could save life\n",
    "* **Topic 8** Empirical theories and approximations\n",
    "* **Topic 9** Signals, life and chemistry\n",
    "* **Topic 9** Probability human interactions (with machines or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Document similarity search in concept-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordToIndex = load_pkl(\"wordToIndex\")\n",
    "\n",
    "def simVectors(querry_indices):\n",
    "    \"\"\"\n",
    "        querry_indices : List of indices of all atomic terms contained in the querry.\n",
    "        \n",
    "        Return a matrix containing for each document, \n",
    "        how much this document is similare to each atomic terms of the querry. \n",
    "        \n",
    "        If we call sim vector many times part of this can be precomputed.\n",
    "    \"\"\"\n",
    "    querry_matrix = np.zeros(u_k.shape)\n",
    "    querry_matrix[querry_indices,:]=u_k[querry_indices,:] \n",
    "    svdt = s_k @ v_k_T\n",
    "    non_normalized_sims = querry_matrix @ s_k @ v_k_T\n",
    "    \n",
    "    # Compute only the norms we care about \n",
    "    # üö® terms_norms shape depend on querry_indices length.\n",
    "    terms_norms = np.linalg.norm(querry_matrix[querry_indices,:],axis=1)\n",
    "    weighted_document_norms = np.linalg.norm(svdt,axis=0)\n",
    "    \n",
    "    #Less computation but also avoid doing 0/0\n",
    "    term_normalized_sims = np.zeros(non_normalized_sims.shape)\n",
    "    term_normalized_sims[querry_indices,:] = (non_normalized_sims[querry_indices,:].T/terms_norms).T\n",
    "    \n",
    "    return term_normalized_sims/weighted_document_norms\n",
    "\n",
    "\n",
    "def searchTerm(t):\n",
    "    \"\"\"\n",
    "        Return the index of all documents sorted by similarity with the term t in decreasing order.\n",
    "        The argument t can be a single world or a the content as a single string of a document that may or may not in our data.\n",
    "    \"\"\"\n",
    "    #Call the cleaner of t, it will chop it and process it the exact same way as descriptions of courses.\n",
    "    querry_words = cleaner(t)\n",
    "    #Get the indices of all the fragment of t we already know. \n",
    "    #If the cleaning of t result in term wenever saw, we just ignore them. \n",
    "    querry_indices = list(filter(lambda x : x >=0,map(lambda x : wordToIndex.get(x,-1),querry_words)))\n",
    "    query_scores = np.sum(simVectors(querry_indices),0)\n",
    "    \n",
    "    return list(reversed(np.argsort(query_scores)))\n",
    "\n",
    "def printSearchResult(search_result):\n",
    "    for r in search_result:\n",
    "        c = coursIndex[r]\n",
    "        print(c,\":\",nameDict[c])\n",
    "n = 10\n",
    "t1 = \"facebook\"\n",
    "print(\"\\nTop\",n,\"course about\",t1)\n",
    "printSearchResult(searchTerm(t1)[:n])\n",
    "t1 = \"markov chains\"\n",
    "print(\"\\nTop\",n,\"course about\",t1)\n",
    "printSearchResult(searchTerm(t1)[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results  are pretty nice as for both facebook and Markov chains we got many result that are actually related to these domain. In comparison with VSM, LSI do a way better job to search for facebook. As only one course actually contain the word facebook (EE-727), VSM only return this one, while LSI is able to return this course as top result plus many other courses about social network.\n",
    "On the other hand, many courses contain the words ‚Äúmarkov chains‚Äù so VSM is able to discover them all, while LSI will tend to also return course that are about something else, most of the time related to Markov chains but sometime not (MGT-526)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Document-document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "$$sim(d_{1},d_2)={Sv_{d_1}^T . Sv_{d_2}^T\\over{\\left\\lVert Sv_{d_1}^T \\right\\rVert \\left\\lVert Sv_{d_2}^T \\right\\rVert}} $$\n",
    "\n",
    "As we want the course that among all our collection is the most similar with some other given course, it may be more efficient to turn this into matrix operations over the whole matrix v. Note that with this method we must not forget that the course that will rank first place will be the course we received as this course is probably highly similar to itself. \n",
    "\n",
    "To simplify our life we can compute the similarity of any document with any other document by first computing $v^N$ the matrix obtained by normalizing the row of the $Sv^T$ matrix. Then we compute ${v^N}^T v^N$ and then we can read the similarity of the document I with the document j in the cell ij or ji of this matrix. This approach is also more efficient if we request similarity much more often that we add or remove courses which may be not true in our case but is a reasonable assumption in a reallife application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mkSimMatrix():\n",
    "    vn = normalized(s_k @ v_k_T,0)\n",
    "    return vn.T @ vn\n",
    "\n",
    "simMatrix = mkSimMatrix()\n",
    "coursToIndex =  {v: k for k, v in coursIndex.items()}\n",
    "\n",
    "def findCoursesSimilarTo(coursId):\n",
    "    \"\"\"\n",
    "        Return the indices of all courses sorted by similarity with the given cours in decreasing order.\n",
    "    \"\"\"\n",
    "    index = coursToIndex[coursId]\n",
    "    sims = simMatrix[index,:]\n",
    "    return list(reversed(np.argsort(sims)[:-1]))\n",
    "\n",
    "n = 5\n",
    "t1 = \"COM-308\"\n",
    "print(\"Top\",n,\"course similare to \",t1)\n",
    "printSearchResult(findCoursesSimilarTo(t1)[:n])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
