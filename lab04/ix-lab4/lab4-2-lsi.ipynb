{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2: Latent semantic indexing\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *Your group letter.*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Name 1*\n",
    "* *Name 2*\n",
    "* *Name 3*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 2 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Donâ€™t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.sparse.linalg import svds\n",
    "from lab04_helper import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Latent semantic indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 300\n",
    "td_matrix = load_sparse_csr(\"occ_matrix.npz\")\n",
    "# SVD decomposition\n",
    "print(td_matrix.shape)\n",
    "u, s, v = svds(td_matrix,k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_k = u[:,:k]\n",
    "v_k_T = v[:k,:]\n",
    "s_k = np.diag(s[:k])\n",
    "print(\"20 biggest eigenvalues : \\n\",s[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix U, each __row__ correspond to a term, and for each term the value index i correspond to how much relevant the ith concept is for this term.\n",
    "In the matrix $V^T$, each __column__ correspond to a document(cours description), and for each document the value at index i correspond to how much relevant the ith concept is for this term. The values on the diagonal of S corespound to the importance of each topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 10\n",
    "wordIndex = load_pkl(\"indexToWord\")\n",
    "coursIndex = load_pkl(\"indexToCourse\")\n",
    "\n",
    "n_topic_relevant_term_index = np.argsort(u_k , 0)[-n:,:]\n",
    "n_topic_relevant_document_index = np.argsort(v_k_T, 1)[:,-n:]\n",
    "\n",
    "allCourses = load_json('data/courses.txt') \n",
    "nameDict =  {t[\"courseId\"]:t[\"name\"] for t in allCourses}\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    print(\"\\n*************************************************\")\n",
    "    print(\"Topic #\",(i+1),\"\\n\")\n",
    "    listPrettyPrint( [\"%s - %.7f\" %(wordIndex[idx], u_k[idx,i])for idx in reversed(n_topic_relevant_term_index[-m:,i])],3)\n",
    "    print(\"\\n\\n\")\n",
    "    for s in [\"%s - %.7f\" % (nameDict[coursIndex[idx]] , v_k_T[i,idx]) for idx in reversed(n_topic_relevant_document_index[i,-m:])] :\n",
    "        print(s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "* **Topic 1** Mostly biologic material and energy\n",
    "* **Topic 2** Use big data sets\n",
    "* **Topic 3** Mostly artifical material and energy\n",
    "* **Topic 4** Bio engneering, circuits and public policy\n",
    "* **Topic 5** Physical principles used by cells\n",
    "* **Topic 6** Things with risk management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Document similarity search in concept-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordToIndex = load_pkl(\"wordToIndex\")\n",
    "\n",
    "def simVectors(querry_indices):\n",
    "    \"\"\"\n",
    "        querry_indices : List of indices of all atomic terms contained in the querry.\n",
    "        \n",
    "        Return a matrix containing for each document, \n",
    "        how much this document is similare to each atomic terms of the querry. \n",
    "        \n",
    "        If we call sim vector many times part of this can be precomputed.\n",
    "    \"\"\"\n",
    "    querry_matrix = np.zeros(u_k.shape)\n",
    "    querry_matrix[querry_indices,:]=u_k[querry_indices,:] \n",
    "    svdt = s_k @ v_k_T\n",
    "    non_normalized_sims = querry_matrix @ s_k @ v_k_T\n",
    "    \n",
    "    # Compute only the norms we care about \n",
    "    # ðŸš¨ terms_norms shape depend on querry_indices length.\n",
    "    terms_norms = np.linalg.norm(querry_matrix[querry_indices,:],axis=1)\n",
    "    weighted_document_norms = np.linalg.norm(svdt,axis=0)\n",
    "    \n",
    "    #Less computation but also avoid doing 0/0\n",
    "    term_normalized_sims = np.zeros(non_normalized_sims.shape)\n",
    "    term_normalized_sims[querry_indices,:] = (non_normalized_sims[querry_indices,:].T/terms_norms).T\n",
    "    \n",
    "    return term_normalized_sims/weighted_document_norms\n",
    "\n",
    "\n",
    "def searchTerm(t):\n",
    "    querry_words = cleaner(t)\n",
    "    querry_indices = list(filter(lambda x : x >=0,map(lambda x : wordToIndex.get(x,-1),querry_words)))\n",
    "    query_scores = np.sum(simVectors(querry_indices),0)\n",
    "    \n",
    "    return list(reversed(np.argsort(query_scores)))\n",
    "\n",
    "def printSearchResult(search_result):\n",
    "    for r in search_result:\n",
    "        c = coursIndex[r]\n",
    "        print(c,\":\",nameDict[c])\n",
    "n = 10\n",
    "t1 = \"facebook\"\n",
    "print(\"\\nTop\",n,\"course about\",t1)\n",
    "printSearchResult(searchTerm(t1)[:n])\n",
    "t1 = \"markov chains\"\n",
    "print(\"\\nTop\",n,\"course about\",t1)\n",
    "printSearchResult(searchTerm(t1)[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Document-document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "$$sim(d_{1},d_2)={Sv_{d_1}^T . Sv_{d_2}^T\\over{\\left\\lVert Sv_{d_1}^T \\right\\rVert \\left\\lVert Sv_{d_2}^T \\right\\rVert}} $$\n",
    "\n",
    "As we want the course that among all our collection is the most similar with some other given course, it may be more efficient to turn this into matrix operations over the whole matrix v. Note that with this method we must not forget that the course that will rank first place will be the course we received as this course is probably highly similar to itself. \n",
    "\n",
    "To simplify our life we can compute the similarity of any document with any other document by first computing $v^N$ the matrix obtained by normalizing the row of the $Sv^T$ matrix. Then we compute ${v^N}^T v^N$ and then we can read the similarity of the document I with the document j in the cell ij or ji of this matrix. This approach is also more efficient if we request similarity much more often that we add or remove courses which may be not true in our case but is a reasonable assumption in a reallife application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mkSimMatrix():\n",
    "    vn = normalized(s_k @ v_k_T,0)\n",
    "    return vn.T @ vn\n",
    "\n",
    "simMatrix = mkSimMatrix()\n",
    "coursToIndex =  {v: k for k, v in coursIndex.items()}\n",
    "\n",
    "def findCoursesSimilarTo(coursId):\n",
    "    index = coursToIndex[coursId]\n",
    "    sims = simMatrix[index,:]\n",
    "    return list(reversed(np.argsort(sims)[:-1]))\n",
    "\n",
    "n = 5\n",
    "t1 = \"COM-308\"\n",
    "print(\"Top\",n,\"course similare to \",t1)\n",
    "printSearchResult(findCoursesSimilarTo(t1)[:n])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
